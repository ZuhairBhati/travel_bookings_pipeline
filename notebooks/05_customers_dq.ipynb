{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f0af63-8710-4c24-ade1-26572c9553d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAVEL BOOKING SCD2 MERGE PROJECT - DATA QUALITY: CUSTOMER DATA VALIDATION\n",
    "# =============================================================================\n",
    "# This notebook performs comprehensive data quality checks on customer data\n",
    "# Purpose: Validates customer data integrity using PyDeequ framework\n",
    "# Data Quality: Checks completeness and business rules for customer attributes\n",
    "# Output: Logs DQ results and raises exceptions for failed validations\n",
    "\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =============================================================================\n",
    "# PARAMETER EXTRACTION WITH DEFAULTS\n",
    "# =============================================================================\n",
    "# Extract widget parameters with fallback defaults for flexibility\n",
    "# arrival_date: Business date for processing (defaults to today)\n",
    "# catalog: Unity Catalog name (defaults to travel_bookings)\n",
    "# schema: Target schema (defaults to default)\n",
    "\n",
    "import datetime as _dt\n",
    "try:\n",
    "    arrival_date = dbutils.widgets.get(\"arrival_date\")\n",
    "except Exception:\n",
    "    arrival_date = _dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "try:\n",
    "    catalog = dbutils.widgets.get(\"catalog\")\n",
    "except Exception:\n",
    "    catalog = \"travel_bookings\"\n",
    "try:\n",
    "    schema = dbutils.widgets.get(\"schema\")\n",
    "except Exception:\n",
    "    schema = \"default\"\n",
    "\n",
    "# =============================================================================\n",
    "# SOURCE DATA PREPARATION\n",
    "# =============================================================================\n",
    "# Load customer data from bronze layer for the specified business date\n",
    "# Filters to current day's data for incremental DQ processing\n",
    "\n",
    "src = spark.table(f\"{catalog}.bronze.customer_inc\").where(F.col(\"business_date\") == F.to_date(F.lit(arrival_date)))\n",
    "\n",
    "# =============================================================================\n",
    "# DATA QUALITY CHECKS DEFINITION\n",
    "# =============================================================================\n",
    "# Define comprehensive DQ checks using PyDeequ framework\n",
    "# hasSize: Ensures data exists (row count > 0)\n",
    "# isComplete: Validates required customer fields are not null\n",
    "# Focus on customer-specific attributes: name, address, email\n",
    "\n",
    "check = (Check(spark, CheckLevel.Error, \"Customer Data Check\")\n",
    "         .hasSize(lambda x: x > 0)\n",
    "         .isComplete(\"customer_name\")\n",
    "         .isComplete(\"customer_address\")\n",
    "         .isComplete(\"email\"))\n",
    "\n",
    "# =============================================================================\n",
    "# DQ EXECUTION AND RESULTS\n",
    "# =============================================================================\n",
    "# Execute DQ checks and capture results for audit logging\n",
    "# Displays results for immediate review and stores for historical tracking\n",
    "\n",
    "result = (VerificationSuite(spark).onData(src).addCheck(check).run())\n",
    "df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "\n",
    "# =============================================================================\n",
    "# DQ RESULTS STORAGE SETUP\n",
    "# =============================================================================\n",
    "# Create operations schema and DQ results table for audit tracking\n",
    "# Stores DQ check results with metadata for monitoring and reporting\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.ops\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.ops.dq_results (\n",
    "  business_date DATE,\n",
    "  dataset STRING,\n",
    "  check_name STRING,\n",
    "  status STRING,\n",
    "  constraint STRING,\n",
    "  message STRING,\n",
    "  recorded_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# DQ RESULTS LOGGING\n",
    "# =============================================================================\n",
    "# Transform and store DQ results with metadata for audit trail\n",
    "# Includes business_date, dataset name, and timestamp for tracking\n",
    "\n",
    "out = (df\n",
    "  .withColumn(\"business_date\", F.to_date(F.lit(arrival_date)))\n",
    "  .withColumn(\"dataset\", F.lit(\"customer_inc\"))\n",
    "  .withColumn(\"recorded_at\", F.current_timestamp()))\n",
    "\n",
    "display(df)\n",
    "\n",
    "out.select(\"business_date\",\"dataset\",\"check\",\"check_status\",\"constraint\",\"constraint_status\",\"constraint_message\",\"recorded_at\") \\\n",
    "   .write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.ops.dq_results\")\n",
    "\n",
    "# =============================================================================\n",
    "# DQ VALIDATION AND ERROR HANDLING\n",
    "# =============================================================================\n",
    "# Validate DQ results and raise exception if any checks failed\n",
    "# Ensures data quality before proceeding to downstream processing\n",
    "\n",
    "if result.status != \"Success\":\n",
    "  raise ValueError(\"DQ failed for customers\")\n",
    "\n",
    "print(\"Customer DQ passed\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_customers_dq",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}